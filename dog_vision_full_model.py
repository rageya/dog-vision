# -*- coding: utf-8 -*-
"""Dog_Vision_Full_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DW30AYcKDQRbNvVSR4fSlI1EFxbVvvZQ

## Tools and Initiating Notebook
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
try:
  # %tensorflow_version only exists in Colab
#   %tensorflow_version 2.x
except Exception:
  pass

#!unzip "/content/drive/MyDrive/DOG-VISION/dog-breed-identification.zip" -d "/content/drive/MyDrive/DOG-VISION/"

import tensorflow as tf
import tensorflow_hub as hub

print("GPU", "Available" if tf.config.list_physical_devices("GPU") else "Not Available")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sklearn

"""## Getting our data Ready (turning into tensors)

Our data has to be in numerical format.
"""

label_csv = pd.read_csv("/content/drive/MyDrive/DOG-VISION/labels.csv")
print(label_csv.describe())
print(label_csv.head())

label_csv.head()

# How many images are there of each breed
label_csv.breed.value_counts().plot.bar(figsize=(20,10))

label_csv.breed.value_counts().median()

from IPython.display import Image
#Image("/content/drive/MyDrive/DOG-VISION/train/0a0c223352985ec154fd604d7ddceabd.jpg")

"""## Getting images and their labels

List of images and their pathnames
"""

label_csv.head()

# Create pathnames from Images ID's

filenames = ["drive/MyDrive/DOG-VISION/train/" + fname + ".jpg" for fname in label_csv['id']]

filenames[:10]

# Checking whether number of filenames matches with number of actual image files

import os
if len(os.listdir("drive/MyDrive/DOG-VISION/train/")) == len(filenames):
  print("Matches to actual data")
else:
  print("Does not matches to actual data")

Image(filenames[9000])

import numpy as np

labels = label_csv['breed'].to_numpy() #convert labels coloumn to numpy Array
labels[:10]

# See if number of labels matches the number of filenames
if len(labels) == len(filenames):
  print("Number of labels matches number of filenames!")
else:
  print("Number of labels does not match number of filenames, check data directories.")

# Finding unique values

unique_breeds = np.unique(labels)
len(unique_breeds)

# Turn a single label into a array of booleans
print(labels[0])
labels[0] == unique_breeds

# Turn every label into a boolean array

boolean_labels = [label == unique_breeds for label in labels]
boolean_labels[:2]

print(len(boolean_labels))

# Turning boolean arrays into integers
print(labels[0]) #original label
print(np.where(unique_breeds == labels[0])) #index where labels occurs
print(boolean_labels[0].argmax()) #index where label occurs in boolean_labels
print(boolean_labels[0].astype(int)) # there will be 1 where actual label occurs

"""## Creating our own Validation Set"""

# Setup X & y variables
X = filenames

y = boolean_labels

len(filenames) == len(boolean_labels)

"""### Starting off with 1000 images and increasing as we progress through"""

# Set a number of 1000 images to use for experimenting
NUM_IMAGES = 1000 #@param {type:"slider", min:1000, max:10000, step:100}
NUM_IMAGES

# Lets split our data into train and validation set
from sklearn.model_selection import train_test_split

# Split them into training and validation of total size NUM_IMAGES

X_train, X_val, y_train, y_val = train_test_split(X[:NUM_IMAGES],
                                                  y[:NUM_IMAGES],
                                                  test_size= 0.2,
                                                  random_state = 42)

len(X_train), len(X_val), len(y_train), len(y_val)

X_train[:5], y_train[:2]

"""## Preprocessing Images (turning images into tensors)

To preprocess our images into Tensors we're going to write a function which does a few things:

1. Takes an image filename as input.
2. Uses TensorFlow to read the file and save it to a variable, image.
3. Turn our image (a jpeg file) into Tensors.
4. Normalize our image (convert color channel values from 0-255 to 0-1)
5. Resize the image to be of shape (224, 224).
6. Return the modified image.
"""

# Convert Image into Numpy Array
from matplotlib.pyplot import imread
image = imread(filenames[42])
image.shape

image #click on 'show data' for viewing the array

image.max(), image.min()

image[:2]

# Turning into tensor

tf.constant(image)[:2]

# Define image size
IMG_SIZE = 224

def process_image(image_path):
  # Read in image file
  image = tf.io.read_file(image_path)
  # Turn the image into numerical Tensor with 3 colour channels (RGB)
  image = tf.image.decode_jpeg(image, channels=3)
  # Convert the colour channel values from 0-225 values to 0-1 values
  image = tf.image.convert_image_dtype(image, tf.float32)
  # Resize the image (224, 244)
  image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])
  return image

"""## Turning our data into Batches

Large amount of data may not fit to the model directly in one go

Therefore we divide all of our data into smaller batches, default = 32
"""

# Create a simple function to return a tuple
def get_image_label(image_path, label):
  image = process_image(image_path)
  return image, label

(process_image(X[42]), tf.constant(y[42]))

# Define the batch size
BATCH_SIZE = 32

def create_data_batches(x, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):
  # Test data do not have labels most of the time..
  if test_data:
    print("Creating test data batches...")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(x)))
    data_batch = data.map(process_image).batch(BATCH_SIZE)
    return data_batch

  # If the data if a valid dataset, we don't need to shuffle it
  elif valid_data:
    print("Creating validation data batches...")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(x), # filepaths
                                               tf.constant(y))) # labels
    data_batch = data.map(get_image_label).batch(BATCH_SIZE)
    return data_batch

  else:
    # Training data needs to be shuffled
    print("Creating training data batches...")
    # Turn filepaths and labels into Tensors
    data = tf.data.Dataset.from_tensor_slices((tf.constant(x), # filepaths
                                              tf.constant(y))) # labels

    # Shuffling pathnames and labels before mapping
    data = data.shuffle(buffer_size=len(x))

    # Create (image, label) tuples
    data = data.map(get_image_label)

    # Turn the data into batches
    data_batch = data.batch(BATCH_SIZE)
  return data_batch

# Creating training and validation data batches

train_data = create_data_batches(X_train, y_train)

val_data = create_data_batches(X_val, y_val, valid_data=True)

# Checking out different attributes of our data batches

train_data.element_spec, val_data.element_spec

"""## Visualising Data Branches

Visualing the above written code..
"""

import matplotlib.pyplot as plt

# Create a function to visualise data branches
def show_25_images(image, labels):
  #Displays a plot of 25 image and labels from a databatch
  plt.figure(figsize=(20,10))
  #Loop through 25
  for i in range(25):
    # Create subplots (5 rows, 5 coloumns)
    ax = plt.subplot(5, 5, i+1)
    #Display an image
    plt.imshow(image[i])
    # Add the image label as the title
    plt.title(unique_breeds[labels[i].argmax()])
    # Turn grid lines off
    plt.axis('off')

train_data

train_images, train_labels = next(train_data.as_numpy_iterator())
show_25_images(train_images, train_labels)

# Visualising validation set
val_images, val_labels = next(val_data.as_numpy_iterator())
show_25_images(val_images, val_labels)

"""## Building A Model

### Preparing Our Input and Outputs..
"""

import tf_keras

# Setup input shape to the model
INPUT_SHAPE = [None, IMG_SIZE, IMG_SIZE, 3] # batch, height, width, colour channels

# Setup output shape of the model
OUTPUT_SHAPE = len(unique_breeds) # number of unique labels

# Setup model URL from TensorFlow Hub (Now at Kaggle)
MODEL_URL = "https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4"

"""### Putting Together in `Keras` Deep Learning Model!

1. Create a function which takes the input shape, output shape and the model we have chosen as parameters
2. Defines the layers in a keras model in a subsequential fashion
3. Compiles the model
4. Builds the model
5. Returns the model
"""

# Create a function which builds a Keras model
def create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE, model_url=MODEL_URL):
  print("Building model with:", MODEL_URL)

  # Setup the model layers
  model = tf_keras.Sequential([
    hub.KerasLayer(MODEL_URL), # Layer 1 (input layer)
    tf_keras.layers.Dense(units=OUTPUT_SHAPE,
                          activation="softmax") # Layer 2 (output layer)
  ])

  # Compile the model
  model.compile(
      loss=tf_keras.losses.CategoricalCrossentropy(), # Our model wants to reduce this (how wrong its guesses are)
      optimizer=tf_keras.optimizers.Adam(), # A friend telling our model how to improve its guesses
      metrics=["accuracy"] # We'd like this to go up
  )

  # Build the model
  model.build(INPUT_SHAPE) # Let the model know what kind of inputs it'll be getting

  return model

# Create a model and check its details
model = create_model()
model.summary()

"""### Creating Callbacks

Callbacks are helper function

Will create 2 callbacks

One for TensorBoard

Second for early stopping to prevent our model from training for too long
"""

# Commented out IPython magic to ensure Python compatibility.
## TensorBoard callback

# Load TensorBoard notebook extension

# %load_ext tensorboard

import datetime

# create a function to build a tensorboard callback
def create_tensorboard_callback():
  # create a log directory for storing the logs
  logdir = os.path.join("drive/MyDrive/DOG-VISION/logs",
                        datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))

  return tf_keras.callbacks.TensorBoard(logdir)

## Early Stopping Callback

early_stopping = tf_keras.callbacks.EarlyStopping(monitor = "val_accuracy",
                                                  patience = 3)

"""## Training Our Model(On subset of data).."""

NUM_EPOCHS = 100 #@param {type:'slider', min : 10, max : 100, step : 10}

'''
How many rounds we want our model to look through the data : EPOCHS
'''

# Create a function which trains the model using `create_model`
# Setup TensorBoard callback
# Call the fit function on our model, passing the training data. validation data, number of epochs(rounds) , callbacks (helper function)
# Return the model

# Build a function to train and return the trained model
def train_model():

  # Create a model
  model = create_model()

  # create a new TensorBoard session everytime we train the model
  tensorboard = create_tensorboard_callback()

  # Fit the model to the data, passing it the callback we created.
  model.fit(x = train_data,
            epochs = NUM_EPOCHS,
            validation_data = val_data,
            validation_freq = 1,
            callbacks = [tensorboard, early_stopping])

  # Return the model
  return model

# Fit the model to the data
model = train_model()

# Commented out IPython magic to ensure Python compatibility.
## Checking the TensorBoard logs

# %tensorboard --logdir drive/MyDrive/DOG-VISION/logs

"""## Making and evaluating predictions using a trained model.."""

# Make predictions on the validation data..
predictions = model.predict(val_data, verbose = 1)
predictions

predictions.shape

np.sum(predictions[0])

# Evaluating first prediction
index = 17
print(predictions[0])
print(f"Max value (probability of prediction): {np.max(predictions[index])}")
print(f"Sum: {np.sum(predictions[index])}")
print(f"Max index: {np.argmax(predictions[index])}")
print(f"Predicted label: {unique_breeds[np.argmax(predictions[index])]}")

# Turn probabilities into their respective labels..
def get_pred_label(prediction_probabilities):
  return unique_breeds[np.argmax(prediction_probabilities)]

pred_label = get_pred_label(predictions[9])
pred_label

# Create a function to unbatch a BatchDataset...

def unbatchify(data):

  images = []
  labels = []

  # Loop through unbathed data
  for image, label in data.unbatch().as_numpy_iterator():
    images.append(image)
    labels.append(unique_breeds[np.argmax(label)])

  return images, labels

val_images, val_labels = unbatchify(val_data)
val_images[0], val_labels[0]

"""### Visualising Predictions..

"""

def plot_pred(prediction_probabilities, labels, images, n = 1):
  # View the prediction and the true value and image for sample n

  pred_prob, true_label, image = prediction_probabilities[n], labels[n], images[n]

  #Get the pred label
  pred_label = get_pred_label(pred_prob)

  # Plot and remove ticks
  plt.imshow(image)
  plt.xticks([])
  plt.yticks([])

  # Change the color according to the predicted value whether it is right or wrong

  if pred_label == true_label:
    color = "green"
  else:
    color = "red"

  # Change plot title to be predicted, probability and truth labe;
  plt.title("{} {:2.0f}% ({})".format(pred_label,
                                    np.max(pred_prob)*100,
                                    true_label),
                                    color = color)

plot_pred(prediction_probabilities=predictions,
          labels = val_labels,
          images = val_images,
          n = 77)

def plot_pred_conf(prediction_probabilities, labels, n = 1):

  pred_prob, true_label = prediction_probabilities[n], labels[n]

  # Get the predicted label
  pred_label = get_pred_label(pred_prob)

  # Find the top 10 predictions confidence indexes
  top_10_pred_indexes = pred_prob.argsort()[-10:][::-1]
  # Find the top 10 prediction confidence values
  top_10_pred_values = pred_prob[top_10_pred_indexes]
  # Find the top 10 prediction labels
  top_10_pred_labels = unique_breeds[top_10_pred_indexes]

  # Setup plot
  top_plot = plt.bar(np.arange(len(top_10_pred_labels)),
                     top_10_pred_values,
                     color = 'grey')
  plt.xticks(np.arange(len(top_10_pred_labels)),
             labels = top_10_pred_labels,
             rotation = 'vertical')

  # Change the color of the true label
  if np.isin(true_label, top_10_pred_labels):
    top_plot[np.argmax(top_10_pred_labels == true_label)].set_color('green')
  else:
    pass

plot_pred_conf(prediction_probabilities=predictions,
               labels=val_labels,
               n=9)

# Let's check a few predictions and their different values
i_multiplier = 0
num_rows = 3
num_cols = 2
num_images = num_rows*num_cols
plt.figure(figsize=(5*2*num_cols, 5*num_rows))
for i in range(num_images):
  plt.subplot(num_rows, 2*num_cols, 2*i+1)
  plot_pred(prediction_probabilities=predictions,
            labels=val_labels,
            images=val_images,
            n=i+i_multiplier)
  plt.subplot(num_rows, 2*num_cols, 2*i+2)
  plot_pred_conf(prediction_probabilities=predictions,
                labels=val_labels,
                n=i+i_multiplier)
plt.tight_layout(h_pad=1.0)
plt.show()

"""## Save and Load Model"""

# create a function to save a model

def save_model(model, suffix=None):

  modeldir = os.path.join("drive/MyDrive/DOG-VISION/models",
                          datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))

  model_path = modeldir + "_" + suffix + ".h5" # Save format of the model
  print(f"Saving model to: {model_path}...")
  model.save(model_path)

  return model_path

# Create a function to load a trained model
import tensorflow_hub as hub

def load_model(model_path):

  print(f"Loading saved model from {model_path}...")
  model = tf_keras.models.load_model(model_path,
                                     custom_objects = {"KerasLayer" : hub.KerasLayer}) # Make sure this is the correct name and import

  return model

# Save our model trained on 1000 images
save_model(model, suffix="1000-images-mobilenetv2-Adam")

# Load a trained model
loaded_1000_image_model = load_model("drive/MyDrive/DOG-VISION/models/20240720-160955_1000-images-mobilenetv2-Adam.h5")

# Evaluating the pre-saved model
model.evaluate(val_data)

# Evaluate the loaded model
loaded_1000_image_model.evaluate(val_data)

"""## Training model on full Dataset!"""

# create a databatch with the full dataset

full_data = create_data_batches(X, y)

full_data

# create a model for full model

full_model = create_model()

# creating full model callbacks

full_model_tensorboard = create_tensorboard_callback()
# As we are training on the full dataset so we cant monitor validation accuracy
full_model_early_stopping = tf_keras.callbacks.EarlyStopping(monitor = "accuracy",
                                                             patience = 3)

# Fit the model on the full data
full_model.fit(x = full_data,
               epochs = NUM_EPOCHS,
               callbacks = [full_model_tensorboard, full_model_early_stopping])

"""## Saving and reloading full model"""

# Save model to file
save_model(full_model, suffix="all-images-Adam")

# Load in the full model
loaded_full_model = load_model('drive/MyDrive/DOG-VISION/models/20240720-164539_all-images-Adam.h5')

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir drive/MyDrive/DOG-VISION/logs

full_model.summary()

"""
## Making predictions on the test Dataset"""

# Load test image file names
test_path = 'drive/MyDrive/DOG-VISION/test/'
test_filenames = [test_path + fname for fname in os.listdir(test_path)]
test_filenames[:10]

test_data = create_data_batches(test_filenames, test_data=True)

# Make predictions on test data batch using the loaded full model
test_predictions = loaded_full_model.predict(test_data,
                                             verbose=1)

# Save predictionsd to numpy array for later use
#np.savetxt('drive/MyDrive/DOG-VISION/test_preds.csv', test_predictions, delimiter=',')

# Load test predictions from the saved csv files
test_predictions = np.loadtxt('drive/MyDrive/DOG-VISION/test_preds.csv', delimiter=',')

test_predictions[:10]

test_predictions.shape

"""## Preparing test data set predictions for Kaggle"""

# Create a Pandas DataFrame
preds_df = pd.DataFrame(columns=['id'] + list(unique_breeds))
preds_df.head()

# Append test image id to predictions dataframes..
test_ids = [os.path.splitext(path)[0] for path in os.listdir(test_path)]
preds_df['id'] = test_ids

preds_df.head()

# Add the predictions probabilities to each dog breed coloumn

preds_df[list(unique_breeds)] = test_predictions
preds_df.head()

# Save our predictions dataframe to csv for submission to kaggle
preds_df.to_csv('/content/drive/MyDrive/DOG-VISION/full_model_submission_MobileNetV2.csv',
                index=False)